from pyspark.sql import SparkSession
from pyspark.sql.functions import split, col, regexp_replace, explode

if __name__ == '__main__':
    spark = SparkSession.builder\
        .appName('Read S3 & Write S3').master('local[*]') \
        .config('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.0') \
        .config('fs.s3a.access.key','<Access_Key>') \
        .config('fs.s3a.secret.key', '<Secret Access Key>') \
        .config('fs.s3a.endpoint', 's3.amazonaws.com')\
        .getOrCreate()

    moviesDf = spark.read.format('csv')\
                        .option('header','true') \
                        .option('inferSchema', 'true') \
                        .option('delimiter', ',')\
                        .load('s3Path')
                        # .load('hdfsPath')
    # moviesDf.show(5, truncate=False)

    splitDf = moviesDf.select('movieId', 'title', 'genres')\
                    .withColumn('new_title', split(col('title'),'\(')[0]) \
                    .withColumn('year', regexp_replace(split(col('title'), '\(')[1], '\)', ''))\
                    .drop('title') \
                    .withColumnRenamed('new_title', 'title') \
                    .select('movieId', 'title', 'year',
                            explode(split('genres', '\|')))
    # splitDf.show(5, truncate=False)

    splitDf.write.mode('overwrite') \
        .save('s3Path')
                # .save('hdfsPath')
    print('*****Data Written to S3 Successfully*****')

